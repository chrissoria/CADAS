{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e738d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11413ce",
   "metadata": {},
   "source": [
    "change country between: PR, DR, CU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81478b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "user = \"Chris\"\n",
    "country = \"CU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc2ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CU_directory = r\"/Users/chrissoria/Google Drive/Other Computers/My Laptop (1)/documents/cadas/data/CADAS data upload/cuba\"\n",
    "DR_directory = r\"/Users/chrissoria/Google Drive/Other Computers/My Laptop (1)/documents/cadas/data/CADAS data upload/Rep Dom\"\n",
    "PR_drectory = r\"/Users/chrissoria/Google Drive/Other Computers/My Laptop (1)/documents/cadas/data/CADAS data upload/Puerto_Rico\"\n",
    "\n",
    "if user == \"Chris\":\n",
    "    if country == \"DR\":\n",
    "        current_directory = DR_directory\n",
    "        path1 = DR_directory+\"/All_CSVs\" #where all CSVs are located\n",
    "        path2 = DR_directory+\"/Sync Files/06.11.23/CSV\" #change to show latest upload\n",
    "    elif country == \"CU\":\n",
    "        current_directory = CU_directory\n",
    "        path1 = CU_directory+\"/All_CSVs\" #where we merge all cumulative CSV files into one file\n",
    "        path2 = CU_directory+\"/10_17_23/CSV\" #where we check to see that the latest batch of data includes all files it should\n",
    "    elif country == \"PR\":\n",
    "        current_directory = PR_directory\n",
    "        path1 = PR_directory+\"/All_CSVs\"\n",
    "        path2 = PR_directory+\"/11_13_23\"\n",
    "elif user == \"Ty\":\n",
    "    current_directory = r\"C:\\Users\\Ty\\Desktop\\DataExport\"\n",
    "else:\n",
    "    print(\"Unknown user. Cannot set directory.\")\n",
    "    current_directory = None\n",
    "\n",
    "if current_directory:\n",
    "    os.chdir(current_directory)\n",
    "    file_list = os.listdir(current_directory)\n",
    "    print(f\"Changed directory to: {current_directory}\")\n",
    "    print(path1)\n",
    "    print(path2)\n",
    "    print(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b2a27c",
   "metadata": {},
   "source": [
    "Received files document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c536c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the list of all files in the latest file upload\n",
    "dir_list1 = os.listdir(path2)\n",
    "\n",
    "# To see which files I received from the data manager\n",
    "newdir_list1 = [x for x in dir_list1 if x.endswith('.epi7')]\n",
    "\n",
    "# To see which files I was able to process with Epi Info tool\n",
    "dir_list2 = os.listdir(path2)\n",
    "newdir_list2 = [x for x in dir_list2 if x.endswith('.csv')]\n",
    "\n",
    "def remove_common(a, b):\n",
    "    for i in a[:]:\n",
    "        if i in b:\n",
    "            a.remove(i)\n",
    "            b.remove(i)\n",
    "\n",
    "    print(\"list1 : \", a)\n",
    "    print(\"list2 : \", b)\n",
    "\n",
    "remove_common(newdir_list1, newdir_list2)\n",
    "\n",
    "words_to_filter = [\"Neighborhood\", \"Door\", \"_informationdoor_\", \"_informationdoorparticipants\", \"Listas\", \"_mainhousehold\", \"_participants\", \"_rosterhousehold\", \"_nonresidentchildren\", \"Cognitva\", \"_cognitive\", \"Cognitive_Scoring\", \"Examen_Físico\", \"_physical_exam\",\"Familiar\", \"_household\", \"Informante\", \"_informant\"]\n",
    "\n",
    "# breaking the longer list into smaller lists\n",
    "filtered_lists = {word: [] for word in words_to_filter}\n",
    "\n",
    "for text in newdir_list2:\n",
    "    for word in words_to_filter:\n",
    "        if word in text:\n",
    "            filtered_lists[word].append(text)\n",
    "\n",
    "# change this number depending on how many tablets worth of information you're receiving\n",
    "for word, filtered_list in filtered_lists.items():\n",
    "    while len(filtered_list) < 6:\n",
    "        filtered_list.append(\"Falta\")\n",
    "\n",
    "Received_Files = pd.DataFrame()\n",
    "\n",
    "for word, filtered_list in filtered_lists.items():\n",
    "    Received_Files[word] = pd.Series(filtered_list)\n",
    "\n",
    "Received_Files.to_excel('Received_Files_Checklist.xlsx', index=False)\n",
    "\n",
    "Received_Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f6d39b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if user == \"Chris\":\n",
    "    if country == \"CU\":\n",
    "        pattern = \"/Users/chrissoria/Google Drive/Other Computers/My Laptop \\(1\\)/documents/cadas/data/CADAS data upload/cuba/All_CSVs/\"\n",
    "    elif country == \"DR\":\n",
    "        pattern = \"/Users/chrissoria/Google Drive/Other Computers/My Laptop \\(1\\)/documents/cadas/data/CADAS data upload/Rep \\Dom/All_CSVs/\"\n",
    "    elif country == \"PR\":\n",
    "        pattern = \"/Users/chrissoria/Google Drive/Other Computers/My Laptop \\(1\\)/documents/cadas/data/CADAS data upload/Puerto_Rico/All_CSVs/\"\n",
    "elif user == \"Ty\":\n",
    "    if country == \"CU\":\n",
    "        pattern = r\"C:\\Users\\Ty\\Desktop\\DataExport\"\n",
    "    elif country == \"DR\":\n",
    "        pattern = r\"C:\\Users\\Ty\\Desktop\\DataExport\"\n",
    "else:\n",
    "    print(\"Unknown user. Cannot set directory.\")\n",
    "    current_directory = None\n",
    "\n",
    "folder_path = path1 #goes through this whole folder\n",
    "\n",
    "####COGNITIVE CHILD#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*_cognitive*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + \"_cognitive_\"\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "cog_child = pd.concat(dfs, ignore_index=True)\n",
    "cog_child = cog_child.sort_values(by='filename', ascending=False)\n",
    "cog_child = cog_child.drop(columns=['filename'])\n",
    "cog_child = cog_child.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "pattern_to_remove = r'=HYPERLINK\\(\"media\\\\([^\"]+)\",\\\"<CLICK HERE>\"\\)'\n",
    "cog_child = cog_child.replace(pattern_to_remove, r'\\1', regex=True)\n",
    "\n",
    "cog_child.to_csv(\"CSV_Merged/Cog_Child.csv\")\n",
    "\n",
    "####COGNITIVE PARENT#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*Cognitiva*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + 'Cognitiva_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "cog_parent = pd.concat(dfs, ignore_index=True)\n",
    "cog_parent = cog_parent.sort_values(by='filename', ascending=False)\n",
    "cog_parent = cog_parent.drop(columns=['filename'])\n",
    "cog_parent = cog_parent.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "cog_parent.to_csv(\"CSV_Merged/Cog_Parent.csv\")\n",
    "\n",
    "####COGNITIVE SCORING#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*Cognitive_Scoring*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + 'Cognitive_Scoring_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "cog_scoring = pd.concat(dfs, ignore_index=True)\n",
    "cog_scoring = cog_scoring.sort_values(by='filename', ascending=False)\n",
    "cog_scoring = cog_scoring.drop(columns=['filename'])\n",
    "cog_scoring = cog_scoring.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "cog_scoring.to_csv(\"CSV_Merged/Cog_Scoring.csv\")\n",
    "\n",
    "####HOUSEHOLD CHILD#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*_household*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + '_household_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "household_child = pd.concat(dfs, ignore_index=True)\n",
    "household_child = household_child.sort_values(by='filename', ascending=False)\n",
    "household_child = household_child.drop(columns=['filename'])\n",
    "household_child = household_child.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "household_child.to_csv(\"CSV_Merged/Household_Child.csv\")\n",
    "\n",
    "####HOUSEHOLD PARENT#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*Familiar*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + 'Familiar_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "household_parent = pd.concat(dfs, ignore_index=True)\n",
    "household_parent = household_parent.sort_values(by='filename', ascending=False)\n",
    "household_parent = household_parent.drop(columns=['filename'])\n",
    "household_parent = household_parent.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "household_parent.to_csv(\"CSV_Merged/Household_Parent.csv\")\n",
    "\n",
    "####PHYS CHILD#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*_physical_exam*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + '_physical_exam_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "phys_child = pd.concat(dfs, ignore_index=True)\n",
    "phys_child = phys_child.sort_values(by='filename', ascending=False)\n",
    "phys_child = phys_child.drop(columns=['filename'])\n",
    "phys_child = phys_child.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "phys_child.to_csv(\"CSV_Merged/Phys_Child.csv\")\n",
    "\n",
    "####PHYS PARENT#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*Examen_Físico*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + 'Examen_Físico_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "phys_parent = pd.concat(dfs, ignore_index=True)\n",
    "phys_parent = phys_parent.sort_values(by='filename', ascending=False)\n",
    "phys_parent = phys_parent.drop(columns=['filename'])\n",
    "phys_parent = phys_parent.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "phys_parent.to_csv(\"CSV_Merged/Phys_Parent.csv\")\n",
    "\n",
    "####INFOR CHILD#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*_informant*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + '_informant_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "infor_child = pd.concat(dfs, ignore_index=True)\n",
    "infor_child = infor_child.sort_values(by='filename', ascending=False)\n",
    "infor_child = infor_child.drop(columns=['filename'])\n",
    "infor_child = infor_child.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "infor_child.to_csv(\"CSV_Merged/Infor_Child.csv\")\n",
    "\n",
    "####INFOR PARENT#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*Informante*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + 'Informante_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "infor_parent = pd.concat(dfs, ignore_index=True)\n",
    "infor_parent = infor_parent.sort_values(by='filename', ascending=False)\n",
    "infor_parent = infor_parent.drop(columns=['filename'])\n",
    "infor_parent = infor_parent.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "infor_parent.to_csv(\"CSV_Merged/Infor_Parent.csv\")\n",
    "\n",
    "####SOCIO CHILD#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*_sociodemographic*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + '_sociodemographic_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "socio_child = pd.concat(dfs, ignore_index=True)\n",
    "socio_child = socio_child.sort_values(by='filename', ascending=False)\n",
    "socio_child = socio_child.drop(columns=['filename'])\n",
    "socio_child = socio_child.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "socio_child.to_csv(\"CSV_Merged/Socio_Child.csv\")\n",
    "\n",
    "####SOCIO PARENT#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*Sociodemográfica*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + 'Sociodemográfica_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "socio_parent = pd.concat(dfs, ignore_index=True)\n",
    "socio_parent = socio_parent.sort_values(by='filename', ascending=False)\n",
    "socio_parent = socio_parent.drop(columns=['filename'])\n",
    "socio_parent = socio_parent.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "socio_parent.to_csv(\"CSV_Merged/Socio_Parent.csv\")\n",
    "\n",
    "####NEIGHBORHOOD#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*Neighborhood*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + 'Neighborhood_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "neighborhood = pd.concat(dfs, ignore_index=True)\n",
    "neighborhood = neighborhood.sort_values(by='filename', ascending=False)\n",
    "neighborhood = neighborhood.drop(columns=['filename'])\n",
    "neighborhood = neighborhood.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "neighborhood.to_csv(\"CSV_Merged/Neighborhood.csv\")\n",
    "\n",
    "####DOOR PARENT#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*Door*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + 'Door_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "door_parent = pd.concat(dfs, ignore_index=True)\n",
    "door_parent = door_parent.sort_values(by='filename', ascending=False)\n",
    "door_parent = door_parent.drop(columns=['filename'])\n",
    "door_parent = door_parent.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "door_parent.to_csv(\"CSV_Merged/Door.csv\")\n",
    "\n",
    "####DOOR CHILD 1#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*_informationdoor_*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + '_informationdoor_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "informationdoor = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "informationdoor = informationdoor.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "informationdoor = informationdoor.sort_values(by='filename', ascending=False)\n",
    "informationdoor = informationdoor.drop(columns=['filename'])\n",
    "informationdoor.to_csv(\"CSV_Merged/InformationDoor.csv\")\n",
    "\n",
    "####DOOR CHILD 2#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*_informationdoorparticipants*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + '_informationdoorparticipants_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "informationdoorparticipants = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "informationdoorparticipants = informationdoorparticipants.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "informationdoorparticipants = informationdoorparticipants.sort_values(by='filename', ascending=False)\n",
    "informationdoorparticipants = informationdoorparticipants.drop(columns=['filename'])\n",
    "informationdoorparticipants.to_csv(\"CSV_Merged/InformationDoorParticipants.csv\")\n",
    "\n",
    "####ROSTER PARENT#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*Listas*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + 'Listas_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "rosters_parent = pd.concat(dfs, ignore_index=True)\n",
    "rosters_parent = rosters_parent.sort_values(by='filename', ascending=False)\n",
    "rosters_parent = rosters_parent.drop(columns=['filename'])\n",
    "rosters_parent = rosters_parent.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "rosters_parent.to_csv(\"CSV_Merged/Roster_Parent.csv\")\n",
    "\n",
    "####ROSTER CHILD 1#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*_mainhousehold*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + '_mainhousehold_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "mainhousehold = pd.concat(dfs, ignore_index=True)\n",
    "mainhousehold = mainhousehold.sort_values(by='filename', ascending=False)\n",
    "mainhousehold = mainhousehold.drop(columns=['filename'])\n",
    "mainhousehold = mainhousehold.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "mainhousehold.to_csv(\"CSV_Merged/MainHousehold.csv\")\n",
    "\n",
    "####ROSTER CHILD 2#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*_participants*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + '_participants_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "participants = pd.concat(dfs, ignore_index=True)\n",
    "participants = participants.sort_values(by='filename', ascending=False)\n",
    "participants = participants.drop(columns=['filename'])\n",
    "participants = participants.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "participants.to_csv(\"CSV_Merged/Participants.csv\")\n",
    "\n",
    "####ROSTER CHILD 3#######\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*_rosterhousehold*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + '_rosterhousehold_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "nonparticipants = pd.concat(dfs, ignore_index=True)\n",
    "nonparticipants = nonparticipants.sort_values(by='filename', ascending=False)\n",
    "nonparticipantrs = nonparticipants.drop(columns=['filename'])\n",
    "nonparticipants = nonparticipants.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "nonparticipants.to_csv(\"CSV_Merged/NonParticipants.csv\")\n",
    "\n",
    "####ROSTER CHILD 4#######\n",
    "\n",
    "dfs = [] \n",
    "\n",
    "for filename in glob.glob(os.path.join(folder_path, '*_nonresidentchildren*.csv')):\n",
    "\n",
    "    print(f\"Processing {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    df['filename'] = filename\n",
    "    pattern_to_remove = pattern + '_nonresidentchildren_'\n",
    "    df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "    df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "    df['filename'] = df['filename'].astype(int)\n",
    "    dfs.append(df)\n",
    "    \n",
    "nonresidentchildren = pd.concat(dfs, ignore_index=True)\n",
    "nonresidentchildren = nonresidentchildren.sort_values(by='filename', ascending=False)\n",
    "nonresidentchildren = nonresidentchildren.drop(columns=['filename'])\n",
    "nonresidentchildren = nonresidentchildren.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "\n",
    "nonresidentchildren.to_csv(\"CSV_Merged/NonResidentChildren.csv\")\n",
    "\n",
    "if country != \"CU\":\n",
    "    dfs = []  # list to hold all dataframes\n",
    "    for filename in glob.glob(os.path.join(folder_path, '*Sangre*.csv')):\n",
    "        print(f\"Processing {filename}\")\n",
    "        df = pd.read_csv(filename)\n",
    "        df['filename'] = filename\n",
    "        pattern_to_remove = pattern + 'Sangre_'\n",
    "        df['filename'] = df['filename'].str.replace(pattern_to_remove, '', regex=True)\n",
    "        df['filename'] = df['filename'].str.replace(r'_.*', '', regex=True)\n",
    "        df['filename'] = df['filename'].astype(int)\n",
    "        dfs.append(df)\n",
    "        \n",
    "    sangre = pd.concat(dfs, ignore_index=True)\n",
    "    sangre = sangre.sort_values(by='filename', ascending=False)\n",
    "    sangre = sangre.drop(columns=['filename'])\n",
    "    sangre = sangre.drop_duplicates(subset='GlobalRecordId', keep='first')\n",
    "    \n",
    "    sangre.to_csv(\"CSV_Merged/Sangre.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
